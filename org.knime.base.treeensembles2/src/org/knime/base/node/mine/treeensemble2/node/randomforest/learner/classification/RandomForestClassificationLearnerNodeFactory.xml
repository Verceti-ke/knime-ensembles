<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE knimeNode PUBLIC "-//UNIKN//DTD KNIME Node 2.0//EN" "http://www.knime.org/Node.dtd">
<knimeNode icon="treeensemble_learner.png" type="Learner">
  <name>Random Forest Learner</name>

  <shortDescription>
    Learns a random forest for classification.
  </shortDescription>

  <fullDescription>
    <intro>
      <p>
        Learns a random forest, which consists of a chosen number of decision trees. Each of the decision tree models is
        learned on a different set of rows (records) and a different set of columns (describing attributes), whereby 
        the latter can also be a bit-vector or byte-vector descriptor (e.g. molecular fingerprint).
        The row sets for each decision tree are created by bootstrapping and have the same size as the original input table.
        For each node of a decision tree a new set of attributes is determined by taking a random sample of size sqrt(m) where
        m is the total number of attributes.
         The output model describes 
        a random forest and is applied in the corresponding predictor node using a simple majority 
        vote. 
      </p>
      <p>
      	This node provides a subset of the functionality of the <i>Tree Ensemble Learner</i> corresponding to a random forest.
      	If you need additional functionality please check out the <i>Tree Ensemble Learner</i>.
      </p>
      <p>
        Experiments have shown the results on different data sets are very similar to the 
        <a href="http://cran.r-project.org/web/packages/randomForest/">Random Forest implementation available in R</a>.
      </p>
      <p>
        The decision tree construction takes place in main memory (all data and all models are kept in memory).
      </p>
      <p>
      	The missing value handling corresponds to the method described <a href="https://github.com/dmlc/xgboost/issues/21">here</a>.
      	The basic idea is to try for each split to send the missing values in every direction and the one yielding the best results (i.e. largest gain)
      	is then used. If no missing values are present during training, the direction of a split that the most records are following is chosen as
      	direction for missing values during testing.
      </p>
       <p>
      	Nominal columns are split in a binary manner. The determination of the split depends on the kind of problem:
      	<ul>
      		<li>For two-class classification problems the method described in section 9.4 of "Classification and Regression Trees" by Breiman et al. (1984) is used.</li>
      		<li>For multi-class classification problems the method described in "Partitioning Nominal Attributes in Decision Trees" by Coppersmith et al. (1999) is used.</li>
      	</ul>
      </p>
    </intro>
    <tab name="Options">
      <option name="Target Column">
        Select the column containing the value to be learned. Rows with missing values in this column will be ignored
        during the learning process.
      </option>
      <option name="Attribute Selection">
        <p>Select the attributes to use learn the model. Two variants are possible.</p>
        <p>
          <i>Fingerprint attribute</i> uses the different bit/byte positions in the selected bit/byte vector as learning 
          attributes (for instance a bit vector of length 1024 is expanded to 1024 binary attributes or 1024 long byte
          vector is expanded to the corresponding number of numeric attributes). All vectors in the selected column must
          have the same length. 
        </p>
        <p>
          <i>Column attributes</i> are nominal and numeric columns used as descriptors. Numeric columns are split in a
          &lt;= fashion; nominal columns are split using set based binary splits (see above for more information). 
        </p>
      </option>
      <option name="Enable Hightlighting (#patterns to store)">
        If selected, the node stores the selected number of rows and allows highlighting them in the node view.
      </option>
      <option name="Save target distribution in tree nodes (memory expensive - 
                    only important for tree view and PMML export)">
        If selected, the model will store the distribution of the target category values in each tree node. This
        information is not relevant for the prediction but is accessible via the tree view or when exporting 
        individual models to PMML. Enabling this option may increase memory consumption considerably.  
      </option>
      <option name="Split Criterion">Choose the <a href="http://en.wikipedia.org/wiki/Decision_tree_learning#Formulae">
        split criterion</a> here. Gini is usually a good choice and is used in CART and the original random forest algorithm
         (as described by Breiman et al); 
        information gain is used in C4.5; information gain ratio normalizes the standard information gain by the split
        entropy to overcome some unfair preference for nominal splits with many child nodes.
        </option>
      <option name="Limit number of levels (tree depth)">
        Number of tree levels to be learned. For instance, a value of 1 would only split the (single) root node
        (decision stump). 
      </option>
      <option name="Minimum node size">Minimum number of records in child nodes. It can be at most half of 
         the minimum split node size (see above). Note, this parameter is currently ignored for nominal splits. 
      </option>
      <option name="Number of models">
        The number of decision trees to learn. A "reasonable" value can range from very few (say 10) to many thousands
        for small data sets with few target category values.
      </option>
      <option name="Use static random seed">
        Choose a seed to get reproducible results.
      </option>
    </tab>
  </fullDescription>
  <ports>
    <inPort index="0" name="Input Data">The data to learn from. It must contain at least one nominal target column and 
      either a fingerprint (bit-vector/byte-vector) column or another numeric or nominal column.
    </inPort>
    <outPort index="0" name="Out-of-bag error estimates">
      The input data with the out-of-bag error estimates, i.e. for each input row the majority vote of all models that 
      did not use the row in the training. If the entire data was used to train the individual models then this
      output will contain the input data with missing prediction and confidence values. The appended columns are 
      equivalent to the columns appended by the corresponding predictor node. There is one additional column
      <i>model count</i>, which contains the number of models used for the voting (number of models not using the row
      throughout the learning.)
    </outPort>
    <outPort index="1" name="Attribute Statistics">
      A statistics table on the attributes used in the different tree learners. Each row represents one training 
      attribute with these statistics: <i>#splits (level x)</i> as the number of models, which use the attribute as 
      split on level <i>x</i> (with level 0 as root split); <i>#candidates (level x)</i> is the number of times an 
      attribute was in the attribute sample for level <i>x</i> (in a random forest setup these samples differ from
      node to node). If no attribute sampling is used <i>#candidates</i> is the number of models. Note, these numbers 
      are uncorrected, i.e. if an attribute is selected on level 0 but is also in the candidate set of level 1 (but 
      will not be split on level 1 because it has been split one level up), the #candidate number will still count 
      the attribute as candidate.
    </outPort>
    <outPort index="2" name="Random Forest Model">The trained model.</outPort>
  </ports>
  <views>
    <view name="Tree Views" index="0">An decision tree viewer for all the trained models. Use the spinner to iterate
      through the different models.
      </view>
  </views>
</knimeNode>

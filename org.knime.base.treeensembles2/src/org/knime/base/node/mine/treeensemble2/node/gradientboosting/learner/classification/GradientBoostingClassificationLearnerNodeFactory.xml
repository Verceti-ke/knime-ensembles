<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE knimeNode PUBLIC "-//UNIKN//DTD KNIME Node 2.0//EN" "http://www.knime.org/Node.dtd">
<knimeNode icon="GradientBoostingLearner.png" type="Learner">
	<name>Gradient Boosting Learner</name>

	<shortDescription>
		Learns a gradient boosting model.
	</shortDescription>

	<fullDescription>
		<intro>
			<p>
				Learns Gradient Boosted Trees with the objective of
				classification. The algorithm uses very shallow regression trees and
				a special form of boosting to build an ensemble of trees.
				The
				implementation follows the algorithm in section
				4.6 of the paper
				"Greedy Function Approximation: A Gradient Boosting
				Machine" by
				Jerome H. Friedman (1999).
			</p>
			<p>
				The used base learner for this ensemble method is a simple
				regression tree as it is used in the
				<i>Tree Ensemble</i>
				,
				<i>Random Forest</i>
				and
				<i>Simple Regression Tree</i>
				nodes.
				Per default a tree is build using binary splits for numeric
				and nominal
				attributes (the later can be changed to multiway splits).
				The built-in missing value handling tries to find the best direction
				for missing values to go to by testing each possible direction and
				selecting the one yielding the best result (i.e. largest gain).
			</p>
		</intro>
		<tab name="Options">
			<option name="Target Column">
				Select the column containing the value to be
				learned. Rows with missing
				values in this column will be ignored
				during the learning process.
			</option>
			<option name="Attribute Selection">
				<p>Select the attributes to use learn the model. Two variants are
					possible.
				</p>
				<p>
					<i>Fingerprint attribute</i>
					uses the different bit/byte positions in the selected bit/byte
					vector as learning
					attributes (for instance a bit vector of length
					1024 is expanded to 1024
					binary attributes or 1024 long byte
					vector
					is expanded to the corresponding number of numeric attributes).
					All
					vectors in the selected column must
					have the same length.
				</p>
				<p>
					<i>Column attributes</i>
					are nominal and numeric columns used as descriptors. Numeric
					columns are split in a
					&lt;= fashion; nominal columns are currently
					split by creating child
					nodes for each of the values.
				</p>
			</option>
			<option name="Limit number of levels (tree depth)">
				Number of tree levels to be learned. For
				instance, a value of 1 would
				only
				split the (single) root node
				(decision stump). For gradient boosted trees usually a depth in the range 4 to 10 is sufficient.
				Larger trees will quickly lead to overfitting.
			</option>
			<option name="Number of models">
				The number of decision trees to learn. A
				"reasonable" value can range
				from very few (say 10) to many thousands
				for small data sets with few target category values.
				Unlike the random forest algorithm, gradient boosted trees tend to overfit if the number of models is set too high and the
				learning rate is not low enough.
			</option>
			<option name="Learning rate">
				The learning rate influences how much influence a single model has on the ensemble result.
				Usually a value of 0.1 is a good starting point but the best learning rate also depends on the number of models.
				The more models the ensemble contains the lower the learning rate has to be.
			</option>
		</tab>
		<tab name="Advanced Options">
			<option name="Use mid points splits (only for numeric attributes)">
				Uses for numerical splits the middle point
				between two class boundaries.
				If unselected the split attribute value
				is the smaller value with "&lt;=" relationship.
			</option>
			<option name="Use binary splits for nominal columns">
				If this option is checked (this is the default), then nominal columns are split in a binary way using set based splits.
				The algorithm for determining the best binary split is described in section 8.8 of "Classification and Regression Trees" by Breiman et al. (1984).
				If this option is unchecked, the algorithm will produce a child for each possible value of the nominal column.
			</option>
			<option name="Alpha">
				Alpha controls what percentage of the data will be considered as outliers. The higher Alpha the smaller the fraction of outliers.
				If Alpha is set to 1.0, the algorithm will consider no point to be an outlier. This is discouraged however because outliers can have
				fatal effects on regression.
			</option>
			<option name="Data Sampling (Rows)">
				The sampling of the data rows for each individual
				tree: If disabled
				each tree learner gets the full data set,
				otherwise
				each tree is learned with a different data sample. A data fraction
				of 1 (=100%) chosen
				"with replacement" is called bootstrapping. For
				sufficiently large data
				sets this bootstrap sample contains
				about 2/3
				different data rows from the input, some of which replicated
				multiple times. Rows which are not used
				in the training of a tree are
				called out-of-bag (see below).
			</option>
			<option name="Attribute Sampling (Columns)">
				Defines the sampling of attributes to learn an
				individual tree. This can
				either be a function based on the
				number of
				attributes (linear fraction or square root) or some absolute
				value.
				The latter can be used in
				conjunction with flow variables to inject
				some other value derived from the
				number of attributes (e.g. Breiman
				suggests to start with the square root of number of attributes but
				also try
				to double/half that number).
			</option>
			<option name="Attribute Selection">
				<p>
					<i>Use same set of attributes for each tree</i>
					describes that the attributes are sampled once for each tree
					and
					this sample is then used to construct the tree.
				</p>
				<p>
					<i>Use different set of attributes for each tree node</i>
					samples a different set of candidate attributes in
					each of the tree
					nodes from which the optimal one is chosen to perform
					the split.
				</p>
			</option>
			<option name="Use static random seed">
				Choose a seed to get reproducible results.
			</option>
		</tab>
	</fullDescription>
	<ports>
		<inPort index="0" name="Input Data">The data to learn from. It must contain
			at least one nominal target column and
			either a fingerprint
			(bit/byte/double vector) column or another numeric or nominal
			column.
		</inPort>
		<outPort index="0" name="Gradient Boosting Model">The trained model.</outPort>
	</ports>
</knimeNode>

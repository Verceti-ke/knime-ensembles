<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE knimeNode PUBLIC "-//UNIKN//DTD KNIME Node 2.0//EN" "http://www.knime.org/Node.dtd">
<knimeNode icon="treeensemble_learner_regression.png" type="Learner">
  <name>Random Forest Learner (Regression)</name>

  <shortDescription>
    Learns a random forest for regression.
  </shortDescription>

  <fullDescription>
    <intro>
      <p>
        Learns a random forest (an ensemble of decision trees) for regression. Each of the regression tree models is
        learned on a different set of rows (records) and/or a different set of columns (describing attributes), whereby 
        the latter can also be a bit/byte/double vector descriptor (e.g. molecular fingerprint). The output model describes an
        ensemble of regression tree models and is applied in the corresponding predictor node using a simple mean
        of the individual predictions. 
      </p>
      <p>
        For a more general description and suggested default parameters see the node description of the classification 
        <i>Random Forest Learner</i>.
      </p>
      <p>
      	This node provides a subset of the functionality of the <i>Tree Ensemble Learner (Regression)</i>. If you need additional
      	functionality, please check out the <i>Tree Ensemble Learner (Regression)</i>
      </p>
    </intro>
    <tab name="Options">
      <option name="Target Column">
        Select the column containing the value to be learned. Rows with missing values in this column will be ignored
        during the learning process.
      </option>
      <option name="Attribute Selection">
		<p>Select the attributes on which the model should be learned. You can choose from two modes.
		</p>
		<p>
			<i>Fingerprint attribute</i>
			Uses a fingerprint/vector (bit, byte and double are possible) column to learn the model by treating
			each entry of the vector as separate attribute (e.g. a bit vector of length 1024 is expanded into 1024 binary attributes).
			The node requires all vectors to be of the same length.
		</p>
		<p>
			<i>Column attributes</i>
			Uses ordinary columns in your table (e.g. String, Double, Integer, etc.) as attributes to learn the model on.
			The dialog allows to select the columns manually (by moving them to the right panel) or via a wildcard/regex selection
			(all columns whose names match the wildcard/regex are used for learning).
			In case of manual selection, the behavior for new columns (i.e. that are not available at the time you configure the node)
			can be specified as either <i>Enforce exclusion</i> (new columns are excluded and therefore not used for learning) or 
			<i>Enforce inclusion</i> (new columns are included and therefore used for learning).
		</p>
	</option>
      <option name="Enable Hightlighting (#patterns to store)">
        If selected, the node stores the selected number of rows and allows highlighting them in the node view.
      </option>
      <option name="Limit number of levels (tree depth)">
        Number of tree levels to be learned. For instance, a value of 1 would only split the (single) root node
        (decision stump). 
      </option>
      <option name="Minimum node size">Minimum number of records in child nodes. It can be at most half of 
         the minimum split node size (see above). Note, this parameter is currently ignored for nominal splits. 
      </option>
      <option name="Number of models">
        The number of decision trees to learn. A "reasonable" value can range from very few (say 10) to many thousands
        for small data sets with few target category values.
      </option>
      <option name="Use static random seed">
        Choose a seed to get reproducible results.
      </option>
    </tab>
  </fullDescription>
  <ports>
    <inPort index="0" name="Input Data">The data to learn from. It must contain at least one numeric target column and 
      either a fingerprint (bit-vector/byte-vector) column or another numeric or nominal column.
    </inPort>
    <outPort index="0" name="Out-of-bag response">
      The input data with the out-of-bag response estimates, i.e. for each input row the mean output of all models that 
      did not use the row in the training. If the entire data was used to train the individual models then this
      output will contain the input data with missing response and response variance values. The appended columns are 
      equivalent to the columns appended by the corresponding predictor node. There is one additional column
      <i>model count</i>, which contains the number of models used for the voting (number of models not using the row
      throughout the learning.)
    </outPort>
    <outPort index="1" name="Attribute Statistics">
      A statistics table on the attributes used in the different tree learners. Each row represents one training 
      attribute with these statistics: <i>#splits (level x)</i> as the number of models, which use the attribute as 
      split on level <i>x</i> (with level 0 as root split); <i>#candidates (level x)</i> is the number of times an 
      attribute was in the attribute sample for level <i>x</i> (in a random forest setup these samples differ from
      node to node). If no attribute sampling is used <i>#candidates</i> is the number of models. Note, these numbers 
      are uncorrected, i.e. if an attribute is selected on level 0 but is also in the candidate set of level 1 (but 
      will not be split on level 1 because it has been split one level up), the #candidate number will still count 
      the attribute as candidate.
    </outPort>
    <outPort index="2" name="Random Forest Model">The trained model.</outPort>
  </ports>
  <views>
    <view name="Tree Views" index="0">An decision tree viewer for all the trained models. Use the spinner to iterate
      through the different models. 
      </view>
  </views>
</knimeNode>
